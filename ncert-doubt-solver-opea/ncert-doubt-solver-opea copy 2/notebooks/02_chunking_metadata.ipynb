{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============ CELL 1: Setup ============\n",
        "!pip install spacy sentence-transformers\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEIh4LqURhbT",
        "outputId": "ad85a616-160b-4617-ae9f-ad557e889cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWmM6EBKRCao",
        "outputId": "70fe60b9-1eaa-4ad3-9762-1850247ac6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/ncert_processed/6_science_english_extracted.json...\n",
            "✓ Saved 162 chunks to /content/drive/MyDrive/ncert_processed/6_science_english_chunks.json\n",
            "Processing /content/drive/MyDrive/ncert_processed/6_science_hindi_extracted.json...\n",
            "✓ Saved 181 chunks to /content/drive/MyDrive/ncert_processed/6_science_hindi_chunks.json\n",
            "\n",
            "✅ Chunking completed!\n",
            "Total chunks: 181\n",
            "Average chunk size: 259.7 tokens\n",
            "\n",
            "Sample chunk:\n",
            "{\n",
            "  \"chunk_id\": \"f4d3f4226a02\",\n",
            "  \"text\": \"\\u0935\\u0926\\u092f\\u0930\\u0925\\u092f \\u0915 \\u0932\\u090f \\u0938\\u0926\\u0936 \\u0907\\u0938 \\u092a\\u0926\\u092f\\u092a\\u0938\\u0924\\u0915 \\u0915 \\u0905\\u0927\\u092f\\u092f\\u0928 \\u0915 \\u092f\\u0924\\u0930 \\u092e \\u092a\\u0939\\u0932 26 5\\u096a) \\u0914\\u0930 \\u092c\\u091d \\u0915 \\u091f\\u092e \\u0938\\u0926\\u0935 \\u0906\\u092a\\u0915 \\u0938\\u0925 \\u0930\\u0939\\u0917 \\u0909\\u0928\\u0939 4 \\u092a\\u0930\\u0936\\u0928 \\u092a\\u091b\\u0928 \\u092c\\u0939\\u0924 \\u0905\\u0927\\u0915 \\u092a\\u0938\\u0926 \\u0939 \\u092c\\u0939\\u0924 \\u092a\\u0930\\u0915\\u0930 \\u0915 5 2 \\u096a9 \\u0921 \\u092a\\u0930\\u0936\\u0928 \\u0909\\u0928\\u0915 \\u0926\\u092e\\u0917 \\u092e \\u0906\\u0924 \\u0939 \\u0914\\u0930 \\u0935 \\u0909\\u0928 \\u092a\\u0930\\u0936\\u0928 \\u0915 \\u0915\\u0915  \\u096e4 2 \\u0905\\u092a\\u0928 \\u0925\\u0932\\u092f \\u092e \\u0938\\u091c\\u0924 \\u091c\\u0924 \\u0939 \\u0915\\u091b \\u092a\\u0930\\u0936\\u0928 \\u0915 \\u0935 \\u0930 \\u0939 \\u0926 \\u0906\\u092a\\u0915 \\u0938\\u0925 \\u092c\\u091f\\u0917, \\u091c\\u0928\\u0939 \\u0906\\u092a \\u0935\\u092d\\u0928\\u0928 \\u0905\\u0927\\u092f\\u092f \\u092e \\u096d \\u091b 0 \\u0967 \\u092a\\u0922\\u0917 \\u091b  ; \\u0915\\u091b \\u092a\\u0930\\u0936\\u0928 \\u0915 \\u0909\\u0924\\u0924\\u0930 \\u092a\\u0939\\u0932 \\u0914\\u0930 \\u092c\\u091d \\u092d \\u0922\\u0922\\u0928 2 6 \\u092e\\u0939\\u0915  \\u0915 \\u092a\\u0930\\u092f\\u0938 \\u0915\\u0930\\u0917 \\u0915\\u092d \\u0909\\u0928\\u0915 \\u0906\\u092a\\u0938 \\u091a\\u0930\\u091a \\u0915 \\u0926\\u0935\\u0930  \\u096b:\\u096d\\u096d \\u0930\\u091c\\u0936\\u0935 \\u092a\\u0930\\u0936\\u0928 \\u0915 \\u0909\\u0924\\u0924\\u0930 \\u092e\\u0932 \\u091c\\u090f\\u0917 \\u0915\\u092d \\u0905\\u092a\\u0928 \\u0938\\u0939\\u092a\\u0920\\u092f, \\u0939  \\u096f \\u0905\\u0927\\u092f\\u092a\\u0915 \\u0914\\u0930 \\u0905\\u092d\\u092d\\u0935\\u0915 \\u0938 \\u091a\\u0930\\u091a \\u0915\\u0930\\u0915 \\u0909\\u0924\\u0924\\u0930  \\u0939 \\u092e\\u0932\\u0917 \\u0907\\u0928 \\u0938\\u092d \\u0915 \\u0939\\u0924 \\u0939\\u090f \\u092d \\u0915\\u091b \\u092a\\u0930\\u0936\\u0928 \\u0910\\u0938 \\u0939\\u0917 \\u0936\\u0936 \\u096e \\u096a \\u091c\\u0928\\u0915 \\u0909\\u0924\\u0924\\u0930 \\u0909\\u092a\\u0932\\u092c\\u0927 \\u0928\\u0939 \\u0939 \\u092a\\u090f\\u0917 \\u0909\\u0928\\u0939 \\u0915\\u091b \\u092a\\u0930\\u092f\\u0917 \\u0938\\u0935\\u092f \\u0915\\u0930\\u0928 \\u0939\\u0917, \\u092a\\u0938\\u0924\\u0915\\u0932\\u092f \\u092e \\u0915\\u0924\\u092c \\u092a\\u0922\\u0928 \\u0939\\u0917 \\u0914\\u0930 \\u092a\\u0930\\u0936\\u0928 \\u0915 \\u0935\\u091c\\u091e\\u0928\\u0915 \\u0915 \\u092a\\u0938 \\u092d\\u091c\\u0928 \\u0939\\u0917 \\u0909\\u0928\\u0915 \\u092a\\u0930\\u0936\\u0928 \\u0915 \\u0909\\u0924\\u0924\\u0930 \\u0939\\u0924 \\u0906\\u092a \\u092f\\u0925\\u0938\\u092d\\u0935 \\u092a\\u0930\\u092f\\u0938 \\u0915\\u0930 \\u0936\\u092f\\u0926 \\u0915\\u091b \\u092a\\u0930\\u0936\\u0928 \\u0910\\u0938 \\u092d \\u0939\\u0917 \\u091c\\u0928\\u0939 \\u0935 \\u0905\\u092a\\u0928 \\u0925\\u0932\\u092f \\u092e \\u092c\\u0927\\u0915\\u0930 \\u092c\\u0921 \\u0915\\u0915\\u0937\\u0913 \\u092e \\u0932 \\u091c\\u090f\\u0917 \\u0906\\u092a\\u0915 \\u0926\\u0935\\u0930 \\u092a\\u091b \\u0917\\u090f \\u092a\\u0930\\u0936\\u0928 \\u0914\\u0930 \\u0909\\u0928\\u0915 \\u092a\\u0930\\u0936\\u0928 \\u0915 \\u0906\\u092a\\u0915 \\u0926\\u0935\\u0930 \\u0926\\u090f \\u0917\\u090f \\u0909\\u0924\\u0924\\u0930, \\u0909\\u0928\\u0939 \\u091c\\u092f\\u0926 \\u0930\\u092e\\u091a\\u0924 \\u0915\\u0930\\u0917 \\u092a\\u0920\\u092f\\u092a\\u0938\\u0924\\u0915 \\u092e \\u0938\\u091d\\u090f \\u0917\\u090f \\u0915\\u091b \\u0915\\u0930\\u092f\\u0915\\u0932\\u092a \\u0915 \\u092a\\u0930\\u0923\\u092e \\u092f \\u0935\\u092d\\u0928\\u0928 \\u0935\\u0926\\u092f\\u0930\\u0925 \\u0938\\u092e\\u0939 \\u0926\\u0935\\u0930 \\u0928\\u0915\\u0932 \\u0917\\u090f \\u0928\\u0937\\u0915\\u0930\\u0937, \\u0926\\u0938\\u0930 \\u0935\\u0926\\u092f\\u0930\\u0925\\u092f \\u0914\\u0930 \\u0905\\u0927\\u092f\\u092a\\u0915 \\u0915 \\u0932\\u090f \\u0930\\u091a\\u0915\\u0930 \\u0939 \\u0938\\u0915\\u0924 \\u0939 \\u0906\\u092a \\u0938\\u091d\\u090f \\u0917\\u090f \\u0915\\u0930\\u092f\\u0915\\u0932\\u092a \\u0915 \\u092a\\u0930 \\u0915\\u0930 \\u0938\\u0915\\u0924 \\u0939 \\u0914\\u0930 \\u0905\\u092a\\u0928 \\u092a\\u0930\\u0923\\u092e \\u092f \\u0928\\u0937\\u0915\\u0930\\u0937 \\u0915 \\u092a\\u0939\\u0932 \\u0914\\u0930 \\u092c\\u091d \\u0915 \\u092d\\u091c \\u0938\\u0915\\u0924 \\u0939 \\u0927\\u092f\\u0928 \\u0930\\u0939 \\u0915 \\u091c\\u0928 \\u0915\\u0930\\u092f\\u0915\\u0932\\u092a \\u092e \\u092c\\u0932\\u0921, \\u0915\\u091a \\u0914\\u0930 \\u0906\\u0917 \\u0915 \\u0906\\u0935\\u0936\\u092f\\u0915\\u0924 \\u0939, \\u0910\\u0938 \\u0915\\u0930\\u092f\\u0915\\u0932\\u092a \\u0915\\u0935\\u0932 \\u0906\\u092a\\u0915 \\u0905\\u0927\\u092f\\u092a\\u0915 \\u0915 \\u0928\\u0930\\u0915\\u0937\\u0923 \\u092e \\u0939 \\u0915\\u090f \\u091c\\u090f \\u0938\\u0935\\u0927\\u0928\\u092f \\u0915 \\u092c\\u0930\\u0924\\u0924 \\u0939\\u090f \\u0938\\u091d\\u090f \\u0917\\u090f \\u0915\\u0930\\u092f\\u0915\\u0932\\u092a \\u0915 \\u0906\\u0928\\u0926 \\u0932\\u091c\\u090f \\u092f\\u0926 \\u0930\\u0916\\u090f \\u0915 \\u0905\\u0917\\u0930 \\u0906\\u092a \\u0938\\u091d\\u090f \\u0917\\u090f \\u0915\\u0930\\u092f\\u0915\\u0932\\u092a \\u0915 \\u092a\\u0930 \\u0928\\u0939 \\u0915\\u0930\\u0924 \\u0924\\u092c \\u092f\\u0939 \\u092a\\u0920\\u092f\\u092a\\u0938\\u0924\\u0915 \\u0906\\u092a\\u0915 \\u0905\\u0927\\u0915 \\u0938\\u0939\\u092f\\u0924 \\u0928\\u0939 \\u0915\\u0930 \\u0938\\u0915\\u0917 \\u092a\\u0939\\u0932 \\u0914\\u0930 \\u092c\\u091d \\u0915 \\u0932\\u090f \\u0906\\u092a \\u0905\\u092a\\u0928 \\u0938\\u091d\\u0935 \\u0915 \\u0928\\u092e\\u0928\\u0932\\u0916\\u0924 \\u092a\\u0924 \\u092a\\u0930 \\u092d\\u091c \\u0938\\u0915\\u0924 \\u0939 \\u0938\\u0935 \\u092e, \\u0905\\u0927\\u092f\\u0915\\u0937, \\u0935\\u091c\\u091e\\u0928 \\u090f\\u0935 \\u0917\\u0923\\u0924 \\u0936\\u0915\\u0937 \\u0935\\u092d\\u0917, \\u090f\\u0928.\\u0938.\\u0908. \\u091f., \\u0936\\u0930 \\u0905\\u0930\\u0935\\u0926 \\u092e\\u0930\\u0917, \\u0928\\u0908 \\u0926\\u0932\\u0932 - 006 \\u0930\\u0935\\u0935079560 2023-24\",\n",
            "  \"grade\": 6,\n",
            "  \"subject\": \"science\",\n",
            "  \"language\": \"hindi\",\n",
            "  \"chapter\": \"Unknown\",\n",
            "  \"page_num\": 9,\n",
            "  \"section\": \"General\",\n",
            "  \"chunk_index\": 0,\n",
            "  \"token_count\": 340,\n",
            "  \"metadata\": {\n",
            "    \"source_file\": \"/content/Vigyan.pdf\",\n",
            "    \"extraction_method\": \"ocr\",\n",
            "    \"confidence\": 1.0\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============ CELL 2: OPEA Chunking Microservice ============\n",
        "import json\n",
        "import spacy\n",
        "from typing import List, Dict\n",
        "from dataclasses import dataclass, asdict\n",
        "import hashlib\n",
        "\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    \"\"\"Data model for text chunks\"\"\"\n",
        "    chunk_id: str\n",
        "    text: str\n",
        "    grade: int\n",
        "    subject: str\n",
        "    language: str\n",
        "    chapter: str\n",
        "    page_num: int\n",
        "    section: str\n",
        "    chunk_index: int\n",
        "    token_count: int\n",
        "    metadata: Dict\n",
        "\n",
        "class OPEAChunkingService:\n",
        "    \"\"\"OPEA Microservice for Semantic Chunking\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 400, overlap: int = 50):\n",
        "        self.chunk_size = chunk_size  # tokens\n",
        "        self.overlap = overlap\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def chunk_extracted_data(self, extracted_file_path: str) -> List[TextChunk]:\n",
        "        \"\"\"Main chunking pipeline\"\"\"\n",
        "        # Load extracted data\n",
        "        with open(extracted_file_path, 'r', encoding='utf-8') as f:\n",
        "            extracted_data = json.load(f)\n",
        "\n",
        "        all_chunks = []\n",
        "\n",
        "        for page_data in extracted_data:\n",
        "            chunks = self._chunk_page(page_data)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "    def _chunk_page(self, page_data: Dict) -> List[TextChunk]:\n",
        "        \"\"\"Chunk a single page\"\"\"\n",
        "        text = page_data['text']\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Split into sentences\n",
        "        sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 10]\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_tokens = 0\n",
        "        chunk_index = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = len(sentence.split())\n",
        "\n",
        "            if current_tokens + sentence_tokens > self.chunk_size and current_chunk:\n",
        "                # Create chunk\n",
        "                chunk_text = ' '.join(current_chunk)\n",
        "                chunks.append(self._create_chunk(\n",
        "                    chunk_text,\n",
        "                    page_data,\n",
        "                    chunk_index\n",
        "                ))\n",
        "                chunk_index += 1\n",
        "\n",
        "                # Keep overlap\n",
        "                overlap_sentences = current_chunk[-(self.overlap // 50):]  # Approximate\n",
        "                current_chunk = overlap_sentences + [sentence]\n",
        "                current_tokens = sum(len(s.split()) for s in current_chunk)\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_tokens += sentence_tokens\n",
        "\n",
        "        # Add remaining chunk\n",
        "        if current_chunk:\n",
        "            chunk_text = ' '.join(current_chunk)\n",
        "            chunks.append(self._create_chunk(\n",
        "                chunk_text,\n",
        "                page_data,\n",
        "                chunk_index\n",
        "            ))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _create_chunk(self, text: str, page_data: Dict, chunk_index: int) -> TextChunk:\n",
        "        \"\"\"Create TextChunk object\"\"\"\n",
        "        # Generate unique ID\n",
        "        chunk_id = hashlib.md5(\n",
        "            f\"{page_data['source_file']}_{page_data['page_num']}_{chunk_index}\".encode()\n",
        "        ).hexdigest()[:12]\n",
        "\n",
        "        # Extract chapter/section (simple heuristic)\n",
        "        chapter, section = self._extract_structure(text)\n",
        "\n",
        "        return TextChunk(\n",
        "            chunk_id=chunk_id,\n",
        "            text=text,\n",
        "            grade=page_data['grade'],\n",
        "            subject=page_data['subject'],\n",
        "            language=page_data['language'],\n",
        "            chapter=chapter,\n",
        "            page_num=page_data['page_num'],\n",
        "            section=section,\n",
        "            chunk_index=chunk_index,\n",
        "            token_count=len(text.split()),\n",
        "            metadata={\n",
        "                'source_file': page_data['source_file'],\n",
        "                'extraction_method': page_data['extraction_method'],\n",
        "                'confidence': page_data.get('confidence', 1.0)\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def _extract_structure(self, text: str) -> tuple:\n",
        "        \"\"\"Extract chapter and section from text (basic heuristic)\"\"\"\n",
        "        # Look for patterns like \"Chapter 1: Numbers\"\n",
        "        import re\n",
        "\n",
        "        chapter_match = re.search(r'Chapter\\s+(\\d+)[:.\\s]+([^\\n]+)', text, re.IGNORECASE)\n",
        "        chapter = chapter_match.group(0) if chapter_match else \"Unknown\"\n",
        "\n",
        "        # Section detection (simplified)\n",
        "        section_match = re.search(r'(\\d+\\.\\d+)\\s+([A-Z][^\\n]+)', text)\n",
        "        section = section_match.group(0) if section_match else \"General\"\n",
        "\n",
        "        return chapter, section\n",
        "\n",
        "    def save_chunks(self, chunks: List[TextChunk], output_path: str):\n",
        "        \"\"\"Save chunks as JSON\"\"\"\n",
        "        chunks_dict = [asdict(chunk) for chunk in chunks]\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chunks_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"✓ Saved {len(chunks)} chunks to {output_path}\")\n",
        "\n",
        "# ============ CELL 3: Process Extracted Files ============\n",
        "chunking_service = OPEAChunkingService(chunk_size=400, overlap=50)\n",
        "\n",
        "extracted_files = [\n",
        "    '/content/drive/MyDrive/ncert_processed/6_science_english_extracted.json',\n",
        "    '/content/drive/MyDrive/ncert_processed/6_science_hindi_extracted.json',\n",
        "\n",
        "    # Add more files\n",
        "]\n",
        "\n",
        "for extracted_file in extracted_files:\n",
        "    print(f\"Processing {extracted_file}...\")\n",
        "\n",
        "    chunks = chunking_service.chunk_extracted_data(extracted_file)\n",
        "\n",
        "    # Save chunks\n",
        "    output_filename = extracted_file.replace('_extracted.json', '_chunks.json')\n",
        "    chunking_service.save_chunks(chunks, output_filename)\n",
        "\n",
        "print(\"\\n✅ Chunking completed!\")\n",
        "\n",
        "# ============ CELL 4: Statistics ============\n",
        "with open(output_filename, 'r') as f:\n",
        "    chunks_data = json.load(f)\n",
        "\n",
        "print(f\"Total chunks: {len(chunks_data)}\")\n",
        "print(f\"Average chunk size: {sum(c['token_count'] for c in chunks_data) / len(chunks_data):.1f} tokens\")\n",
        "print(f\"\\nSample chunk:\")\n",
        "print(json.dumps(chunks_data[10], indent=2))"
      ]
    }
  ]
}